{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USED CAR PRICE PREDICTION  \n",
    "\n",
    "**Submitted by Kommuri Raju**\n",
    "\n",
    "**GCD Student, INSAID**\n",
    "\n",
    "**Batch: May 9, 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Introduction](#section1)<br>\n",
    "2. [Problem Statement](#section2)<br>\n",
    "3. [Library Installments](#section3)<br>\n",
    "    - 3.1 [Installing Libraries](#section301)<br>\n",
    "    - 3.2 [Importing Libraries](#section302)<br>\n",
    "4. [Data Acquisition & Descriptions](#section4)<br>\n",
    "    - 4.1 [Data Description](#section401)<br>\n",
    "    - 4.2 [Data Information](#section402)<br>\n",
    "5. [Data Pre-processing](#section5)<br>\n",
    "    - 5.1 [Pre-profiling Report](#section501)<br>\n",
    "    - 5.2 [Data Cleaning](#section502)<br>\n",
    "    - 5.3 [Post-Profiling Report](#section503)<br>    \n",
    "6. [Exploratory Data Analysis](#section6)<br>\n",
    "    - 6.1 [Analysis of Features w.r.t.Target variable](#section601)<br>\n",
    "7. [Data Post-processing](#section7)<br>\n",
    "    - 7.1 [Handling Categorical columns](#section701)<br>\n",
    "    - 7.2 [Feature Extraction](#section702)<br>\n",
    "    - 7.3 [Linear Regression](#section703)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section1></a>\n",
    "## 1. Introduction\n",
    "\n",
    "**Company Introduction**\n",
    "\n",
    "- **SWIPECAR**, is an American company that **buys and sells second hand cars.**\n",
    "- **They initiated their business in the late 80s and have gained huge popularity over the years.**\n",
    "- **The company clients are local and foreign customers who seek to buy and sell second hand cars.**\n",
    "\n",
    "**Current Scenario**\n",
    "- Company has started **facing loss in business due to the technical advancements.**\n",
    "- There are **several competitors in the market who have been using enhanced techniques.**\n",
    "- The **company is pretty old and they have been using traditional measures to estimate old cars prices.**\n",
    "- These **traditional measures include weight analysis, condition of parts and build year.**\n",
    "- They are looking for a **more robust way to estimate the price of old cars.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section2></a>\n",
    "## 2. Problem Definition\n",
    "\n",
    "**The current process suffers from the following problems:**\n",
    "\n",
    "- They have been using manual traditional measures to estimate old cars prices.\n",
    "- These measures are **time consuming and not accurate.**\n",
    "- Company is looking for a robust way to estimate the prices of used cars.\n",
    "\n",
    "Recently they got to know about data scientists who help businesses to sort out such issues.They decided to hire a team of data scientists. Consider you are one of them.\n",
    "\n",
    "**Your Role**\n",
    "- You will be given a set of data to predict the prices of used cars based on their features.\n",
    "- You need to find an automated way to get rid of their manual work.\n",
    "- Your task is to build a regression model using the provided data.\n",
    "\n",
    "**Project Deliverables**\n",
    "- Deliverable: Predict the prices of used cars.\n",
    "- Machine Learning Task: Regression\n",
    "- Target Variable: Price\n",
    "- Win Condition: N/A (best possible model)\n",
    "\n",
    "**Evaluation Metric**\n",
    "- The model evaluation will be based on the RMSE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section3></a>\n",
    "## 3. Library Installments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section301></a>\n",
    "### 3.1 Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datascience                                         # Package that is required by pandas profiling\n",
    "!pip install -q pandas-profiling                                    # Library to generate basic statistics about data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section302></a>\n",
    "### 3.2 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd                                                 # Importing package pandas (For Panel Data Analysis)\n",
    "from pandas_profiling import ProfileReport                          # Import Pandas Profiling (To generate Univariate Analysis)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np                                                  # Importing package numpys (For Numerical Python)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt                                     # Importing pyplot interface to use matplotlib\n",
    "import seaborn as sns                                             # Importing seaborn library for interactive visualization\n",
    "%matplotlib inline\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import scipy as sp                                                  # Importing library for scientific calculations\n",
    "#-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section4></a>\n",
    "## 4. Data Acquisition & Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dataset is divided into two parts:\n",
    "\n",
    "**Train Set:**\n",
    "\n",
    "| Records | Features | Target Variable |\n",
    " | :-- | :-- | :-- |\n",
    "|**181**|**27**|**Price**|\n",
    "\n",
    "\n",
    "**Test Set:**\n",
    "\n",
    "| Records | Features | Predicted Variable |\n",
    "| :-- | :-- | :-- |\n",
    "| **20**|**26**|**Price** |\n",
    "\n",
    "\n",
    "**The model developmet for price prediction will be done in Jupyter Notebook.**\n",
    "\n",
    "\n",
    "|Id|Feature|Description|\n",
    "|:--|:--|:--|\n",
    "|01| ID |Feature uniquely identifying each record| \n",
    "|02| symboling | Degree to which the auto is riskier than its price indicates|  \n",
    "|03| normalized losses | Relative average loss payment per insured vehicle year| \n",
    "|04| make| Make of the car|   \n",
    "|05| fuel-type| Type of fuel consumed by the car|\n",
    "|06| Aspiration| Type of internal combustion engine used|\n",
    "|07| num-of-doors| Number of doors available in the car|\n",
    "|08| body-style| Body style of car|\n",
    "|09| drive-wheels | Drive wheel of car|\n",
    "|10| engine-location| Location of engine in car|\n",
    "|11| wheel-base| Distance between the centres of the front and rear wheels|\n",
    "|12| length| Length of the car|\n",
    "|13|post-code| Postal Code of the customer| \n",
    "|14| width | Width of the car.|  \n",
    "|15| height | Height of the car| \n",
    "|16| curb-weight| Total mass of a vehicle with standard equipment|   \n",
    "|17| engine-type| Type of engine used in the car|\n",
    "|18| num-of-cylinders| Number of cylinders used in the car|\n",
    "|19| engine-size| Size of the engine used in the car|\n",
    "|20| fuel-system| Type of fuel system used in the car|\n",
    "|21| bore | Diameter of each cylinder in the piston engine|\n",
    "|21| stroke| Full travel of the piston along the cylinder, in either direction|\n",
    "|22| compression-ratio| Volume of the cylinder and the combustion chamber when the piston is at the bottom, and the volume of the combustion chamber when the piston is at the top|\n",
    "|23| horsepower| The power an engine produces by a car|\n",
    "|24| peak-rpm | The max power produced by engine in terms of revolutions per minute|  \n",
    "|25| city-mpg | City mileage per gallon rating of car| \n",
    "|26| highway-mpg| Highway mileage per gallon rating of car|   \n",
    "|27| price| Price of the car |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the Train dataset #\n",
    "missing_values = ['n.a','?','na','NA']                    # Data file contains \"?\" sign as values in some of the columns\n",
    "data = pd.read_csv(\"TrainData-cars price-Narayan.csv\",na_values = missing_values)\n",
    "print('Data Shape: ', data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_final = pd.read_csv(\"TestData-Cars price-Narayan.csv\",na_values = missing_values)\n",
    "print('Data Shape:', data_final.shape)\n",
    "print(data_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section401></a>\n",
    "### 4.1 Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section402></a>\n",
    "### 4.2 Data Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section5></a>\n",
    "## 5. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data == 0 ).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_final == 0 ).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations :\n",
    " - Training Data Set has 59 0s in Column **symboling** column which is fine as symboling values range from -3 to +3\n",
    " - Taining Data Set also has *missing values** or \"?\" in Columns **'normalized losses','num-of-doors','bore','stroke','horsepower' and 'peak-ram'.** \n",
    " - Training Data Set has 6 **missing values** in Column **normalized losses** column \n",
    " - Cleaning of data is done by filling **0's,'?' and BLANKS** with MEAN or MEDIAN or MODE values of that particular column as **DEEMED FIT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section501></a>\n",
    "### 5.1 Pre-profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_Train = ProfileReport(df = data, minimal = True)\n",
    "\n",
    "print('Pre-Profiling Accomplished!')\n",
    "profile_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_Test = ProfileReport(df = data_final, minimal = True)\n",
    "\n",
    "print('Pre-Profiling Accomplished!')\n",
    "profile_Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section502></a>\n",
    "### 5.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['normalized-losses'].median())\n",
    "print(data['normalized-losses'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data['num-of-doors'].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['bore'].median())\n",
    "print(data['bore'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['stroke'].median())\n",
    "print(data['stroke'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['horsepower'].median())\n",
    "print(data['horsepower'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['peak-rpm'].median())\n",
    "print(data['peak-rpm'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normalized-losses'].fillna(value = 115.0, inplace = True)\n",
    "data['bore'].fillna(value = 3.33, inplace = True)\n",
    "data['stroke'].fillna(value = 3.2745, inplace = True)\n",
    "data['horsepower'].fillna(value = 95.0, inplace = True)\n",
    "data['peak-rpm'].fillna(value = 5100.0, inplace = True)\n",
    "data['num-of-doors'].fillna(value = 'four', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_final['normalized-losses'].median())\n",
    "print(data_final['normalized-losses'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final['normalized-losses'].fillna(value = 128.0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handling Duplicate data\n",
    "#print('Contains Duplicate Rows?',data_trn.duplicated().any())\n",
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final[data_final.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations \n",
    "\n",
    "- **No missing value** in the dataset.\n",
    "- **No Duplicated value** in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section503></a>\n",
    "### 5.3 Post-profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_profile_data = ProfileReport(df = data,explorative = True)\n",
    "\n",
    "print('Post-Profiling Accomplished!')\n",
    "post_profile_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_profile_data_final = ProfileReport(df = data,explorative = True)\n",
    "\n",
    "print('Post-Profiling Accomplished!')\n",
    "post_profile_data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data is clean so we are good to go for Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section6></a>\n",
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    " - There is **Skewness** in the target variable.\n",
    " - We have to **transform** it so it can be used for **model building.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section601></a>\n",
    "### 6.1 Analysis of features w.r.t. Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manufacturer having highest Car count\n",
    "fig = plt.figure(figsize = (10,7))\n",
    "data['make'].value_counts().plot(kind = 'barh', cmap = 'Dark2')\n",
    "plt.ylabel('Brand Name', size = 14)\n",
    "plt.xlabel('Car count', size = 14)\n",
    "plt.title('Maker having Highest Car count', size = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No of cars having four doors\n",
    "figure = plt.figure(figsize =(15,7))\n",
    "data['num-of-doors'].value_counts().plot(kind = 'barh',color = 'green')\n",
    "plt.xlabel('Car count', size = 14)\n",
    "plt.ylabel('Car Doors', size = 14)\n",
    "plt.title('No of cars with door count', size = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drive wheel counts for Cars\n",
    "figure = plt.figure(figsize = (10,7)) \n",
    "data['drive-wheels'].value_counts().plot( kind= 'bar', cmap = 'Dark2')\n",
    "plt.xlabel('No of Drive wheels', size = 14)\n",
    "plt.ylabel('Car Count', size = 14)\n",
    "plt.title('Cars having Drive wheel count', size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car counts with Dree of risk\n",
    "figure = plt.figure(figsize = (10,7))\n",
    "data['symboling'].value_counts().plot(kind = 'barh', cmap = 'Set2')\n",
    "plt.xlabel('Car Count', size = 14)\n",
    "plt.ylabel('Risk count', size = 14)\n",
    "plt.title('Risk profile', size = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Expensive Car\n",
    "figure = plt.figure(figsize = (20,8))\n",
    "make_price = data.groupby('make')['price'].mean()   # value_counts().sort_values(ascending = False)[:15]\n",
    "\n",
    "make_price.plot.bar(color = 'tomato')\n",
    "plt.xlabel('Car Company', size = 14)\n",
    "plt.ylabel('Car Price', size = 14)\n",
    "plt.title('Most Expensive Car', size = 16)\n",
    "plt.xticks(size = 12)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Matrices\n",
    "sns.pairplot(data= data[['price','city-mpg','highway-mpg','horsepower','engine-size','curb-weight']], height = 2, aspect = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (20,8))\n",
    "HeatMap = sns.heatmap(data.corr(),annot = True,cmap = 'coolwarm',vmin = 0, vmax = 1,linecolor = 'black',linewidths = 1)\n",
    "HeatMap.set_title('Correlation HeatMap', fontdict = {'fontsize':16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[['highway-mpg', 'city-mpg','horsepower','engine-size','curb-weight','peak-rpm']].plot(kind='box', figsize=(10, 7), vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    " - **Price** is having **Strong Positive correlation** with **curb-weight (0.84), engine-size (0.87) and horsepower (0.8).**\n",
    " - **Price** is having **Strong Negative correlation** with **city-mpg (0.68) and highway-mpg (0.7).**\n",
    " - **wheel-base** is having HIGH CORRELATION with **height, width and curb-weight**\n",
    " - **engine-size** is having HIGH CORRELATION with **horsepower**\n",
    " - **bore** is having HIGH CORRELATION with **curb-weight, length**\n",
    " - **city-mpg** is having HIGH CORRELATION with **highway-mpg**\n",
    " - The  above mentioned INDEPENDENT VARIABLES, are resulting in **Multicollinearity.**\n",
    " - Some Outliers are still present in the feature columns of dataset, for which we will **apply Transformations**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section7></a>\n",
    "## 7. Data Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Machine Learning Algorithm** works with **numerical** values.\n",
    " - So, it is necessary to **convert the categorical columns into Numerical columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section701></a>\n",
    "### 7.1 Handling Categorical columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) Categorical Columns : \n",
    "make, fuel-type, aspiration, num-of-doors, body-style, drive-wheels, engine-location, engine-type, num-of-cylinders, fuel-system.\n",
    "\n",
    "### (B) Numerical Columns:\n",
    "ID, symboling, normalized-losses, wheel-base, length, width, height, curb-weight, bore, stroke, compression- ratio, horsepower, peak-rpm, city-mpg, highway-mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new Dataframe\n",
    "data_categorical = data[['make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location',\n",
    "                         'engine-type','num-of-cylinders','fuel-system']]\n",
    "data_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "data_categorical = data_categorical.apply(LabelEncoder().fit_transform)\n",
    "data_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_numerical = data[['symboling', 'normalized-losses','wheel-base','curb-weight','engine-size','highway-mpg', 'price' ]]\n",
    "data_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- We are taking only ** 7 Numerical Value** Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = pd.concat([data_categorical,data_numerical], axis=1)\n",
    "data_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIF ==> Variance Inflation Factor\n",
    "- **Checking Multicollinearity using VIF**\n",
    "- **Supports only NUMERICAL values, STRING and LOGICAL COLUMNS to be dropped**\n",
    "- **The values cannot be BLANK or NaN**\n",
    "- **VIF or Variable Inflation Factor is used to identify the correlation of 1 INDEPENDENT VARIABLE(PREDICTOR) with a group of OTHER VARIABLES(PREDICTORS)**\n",
    "- **VIF is the RECIPROCAL of TOLERANCE VALUE**\n",
    "- **A RULE OF THUMB commonly used in PRACTICE is; if VIF < 3 it is IDEAL and if  VIF > 10, you have HIGH MULTICOLLINEARITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_intermediate = data[['ID', 'symboling', 'normalized-losses', 'wheel-base', 'length', 'width', 'height',\n",
    "       'curb-weight', 'engine-size', 'bore', 'stroke', 'compression-ratio', 'horsepower',\n",
    "       'peak-rpm', 'city-mpg', 'highway-mpg', 'price' ]]\n",
    "data_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_intermediate = pd.concat([data_categorical,data_intermediate], axis=1)\n",
    "data_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_intermediate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "  # Calculating VIF\n",
    "  vif = pd.DataFrame()\n",
    "  vif[\"variables\"] = X.columns\n",
    "  vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "  return(vif)\n",
    "\n",
    "c = data_intermediate.iloc[::-1]\n",
    "calc_vif(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = data_model.drop(['price'],axis = 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['price']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train cases as below')\n",
    "print('x_train shape: ',x_train.shape)\n",
    "print('y_train shape: ',y_train.shape)\n",
    "print('\\nTest cases as below')\n",
    "print('x_test shape: ',x_test.shape)\n",
    "print('y_test shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = section703></a>\n",
    "### 7.3 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, we will use these 4 steps :** \n",
    "\n",
    " - (a) Load the algorithm \n",
    " - (b) Instantiate and Fit the model/Train the model using the training dataset\n",
    " - (c) Prediction on the test set\n",
    " - (d) Evaluate the model - Calculate RMSE the GO-TO METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lr_model.predict(x_train)  \n",
    "y_pred_test = lr_model.predict(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Metrics for Evaluation of the LINEAR MODEL before HYPER PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "MAE_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "MAE_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "print('MAE for training set is {}'.format(MAE_train))\n",
    "print('MAE for test set is {}'.format(MAE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_train = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))\n",
    "print('RMSE for training set is {}'.format(RMSE_train))\n",
    "print('RMSE for test set is {}'.format(RMSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = metrics.r2_score(y_train, y_pred_train)\n",
    "r2_test = metrics.r2_score(y_test, y_pred_test)\n",
    "print('R-Squared of train data:',r2_train)\n",
    "print('R-Squared of test data:',r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As there is a huge difference between the train score and test score, the model is overfitted. To over come this issue we will use the hyper parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RandomizedSearchCV for Parameters' Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# n(*, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best = LinearRegression(fit_intercept=False,\n",
    "    normalize=True,\n",
    "    copy_X=False,\n",
    "    n_jobs=1,)\n",
    "param_dict = {\"fit_intercept\" :['False'],\n",
    "              \"normalize\": ['False'],\n",
    "              \"copy_X\": ['True','False'] }\n",
    "#clf = RandomizedSearchCV(lr_model,param_dict)\n",
    "#clf.fit(x_train,y_train)\n",
    "rs = RandomizedSearchCV(estimator=lr_best,param_distributions = param_dict,n_iter = 20,n_jobs= -1,random_state=0,)\n",
    "topmost = rs.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topmost.best_params_)\n",
    "print(topmost.best_score_)\n",
    "print(topmost.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_train_best = topmost.predict(x_train)  \n",
    "\n",
    "y_pred_test_best = topmost.predict(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Metrics for Evaluation of the LINEAR MODEL after HYPER PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE is the GO-TO Metric here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train_best = metrics.r2_score(y_train, y_pred_train_best)\n",
    "r2_test_best = metrics.r2_score(y_test, y_pred_test_best)\n",
    "print('R-Squared of train data:',r2_train_best)\n",
    "print('R-Squared of test data:',r2_test_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_train, y_pred_train_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have also calculated other Metrics viz. \n",
    "### MAE (Mean Average Error)\n",
    "### MLSE (Mean squared logarithmic Error) --- if there are NEGATIVE values in the DataSet then we cannot APPLY this SCORE\n",
    "### R2 or R-SQUARED ERROR\n",
    "### MAPE (Mean Average Percentage Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_train = metrics.mean_absolute_error(y_train, y_pred_train_best)\n",
    "MAE_test = metrics.mean_absolute_error(y_test, y_pred_test_best)\n",
    "print('MAE for training set is {}'.format(MAE_train))\n",
    "print('MAE for test set is {}'.format(MAE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLSE_train = metrics.mean_squared_log_error(y_train, y_pred_train_best)\n",
    "MLSE_test = metrics.mean_squared_log_error(y_test,y_pred_test_best)\n",
    "print('Mean squared logarithmic Error(RMLSE) of train data:',MLSE_train)\n",
    "print('Mean squared logarithmic Error(RMLSE) of test data:',MLSE_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = metrics.r2_score(y_train, y_pred_train_best)\n",
    "r2_test = metrics.r2_score(y_test, y_pred_test_best)\n",
    "print('R-Squared of train data:',r2_train)\n",
    "print('R-Squared of test data:',r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Test File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate out the Categorical and Numerical Columns in the Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = data_final['ID']\n",
    "data_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission1 = data_final['ID']\n",
    "data_submission1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_categorical = data_final[['make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels',\n",
    "                                     'engine-location','engine-type','num-of-cylinders','fuel-system']]\n",
    "data_final_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_numerical = data_final[['symboling', 'normalized-losses','wheel-base','curb-weight','engine-size','highway-mpg']]\n",
    "data_final_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_categorical = data_final_categorical.apply(LabelEncoder().fit_transform)\n",
    "data_final_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_model = pd.concat([data_final_categorical,data_final_numerical], axis=1)\n",
    "data_final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_final = lr_model.predict(data_final_model)\n",
    "y_pred_test_final\n",
    "y_pred_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_final_topmost = topmost.predict(data_final_model)\n",
    "y_pred_test_final_topmost\n",
    "y_pred_test_final_topmost.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the array into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_final = pd.DataFrame(y_pred_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_final_topmost = pd.DataFrame(y_pred_test_final_topmost)\n",
    "y_pred_test_final_topmost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the submission file which should have only two columns viz. the KEY/INDEX column(ID) and TARGET column(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = pd.concat([data_submission,y_pred_test_final], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_1 = pd.concat([data_submission1,y_pred_test_final_topmost], axis = 1)\n",
    "submission_file_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To write the final data to the submission files which are .csvs without HEADER and INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.to_csv('Used_Car_Price_Prediction_Submission_Raj.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_1.to_csv('Used_Car_Price_Prediction_Submission_Raj_Tuned.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THANK YOU !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
